{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bbd4984-5d42-472a-b23e-81d5f180672b",
   "metadata": {},
   "source": [
    "# Needed dependencies:\n",
    "***beautifulsoup4, requests, ollama library(pypi), [ollama](https://ollama.com/) and [llama3.2](https://ollama.com/library/llama3.2)***\n",
    "\n",
    "Have ollama running the desired model with the cmd command- ollama run llama3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea5060c-163a-4a64-9915-2db81dd0d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 requests ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6a8a2-2349-4969-94fb-5cbd813658eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ollama import chat\n",
    "\n",
    "def fetch_article(article_url):\n",
    "    try:\n",
    "        response = requests.get(article_url)\n",
    "        response.raise_for_status() \n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract all paragraph (<p>) tags\n",
    "        fetched_paragraphs = soup.find_all('p')\n",
    "        article_content = \" \".join([p.get_text(strip=True) for p in fetched_paragraphs])\n",
    "\n",
    "        # Check if any paragraphs were found\n",
    "        if not article_content.strip():\n",
    "            raise Exception(\"No content found in <p> tags.Try another website\")\n",
    "\n",
    "        return article_content\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(\"Couldn't fetch the article. Try another one with a valid link\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Define the URL of the article you want to summarize. I decided to implement it this way because I don't want the user input() to clutter the ai message\n",
    "    custom_url = \"https://www.bbc.com/news/technology-65855333\"  # example url\n",
    "    \n",
    "    # Fetch the article content\n",
    "    article_text = fetch_article(custom_url)\n",
    "    \n",
    "    # Define the summary sentence count\n",
    "    summary_count = 3\n",
    "    \n",
    "    # Make the call to the LLM (Ollama) for summarization\n",
    "    llama_response = chat(\n",
    "        model='llama3.2',\n",
    "        messages=[{\"role\": \"system\", \"content\": \"Provide the summary with coherent connected sentences with no other outputs or special characters.\"},\n",
    "                  {'role': 'user', 'content': f'Summarize this article with {summary_count} short concise sentences and make an appropriate title for it : {article_text}'}],\n",
    "    )\n",
    "    \n",
    "    # Print the summarization and title from LLM\n",
    "    print(llama_response['message']['content'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bf6518-f46d-4331-8438-3a8131e8f7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
