{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb21dfd-d3ea-4f44-807f-e22772254282",
   "metadata": {},
   "source": [
    "# Using langchain's chatollama and langchain's streaming\n",
    "## *beautifulsoup4, requests, ollama library(pypi), langchain-ollama, [ollama](https://ollama.com/) and [llama3.2](https://ollama.com/library/llama3.2)*\n",
    "### have ollama running - ollama serve/ ollama run llama3.2\n",
    "\n",
    "This code has the same functionality as [task1](https://github.com/krisx2/BlackDeep-jobTasks/blob/main/task1.ipynb) but I'm using langchain to stream and adjust temperature. Due to the streaming and the use of langchain this code will perform better on large articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcff25d-a75e-4247-98a5-1ed1ce60eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f87d52-c7be-421f-a91c-43ba2a16cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "custom_article_url = \"https://www.bbc.com/news/technology-65855333\" # No need for user input()\n",
    "sentence_count = 4           # for longer or shorter summary\n",
    "\n",
    "def fetch_article(article_url):\n",
    "    try:\n",
    "        response = requests.get(article_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract all paragraph (<p>) tags\n",
    "        fetched_paragraphs = soup.find_all('p')\n",
    "        article_content = \" \".join([p.get_text(strip=True) for p in fetched_paragraphs])\n",
    "\n",
    "        # Check if any paragraphs were found\n",
    "        if not article_content.strip():\n",
    "            raise Exception(\"No content found in <p> tags.Try another website\")\n",
    "\n",
    "        return article_content\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Couldn't fetch the article. Try another one with a valid link. Error {e}\")\n",
    "#starting the llm\n",
    "llm = ChatOllama(\n",
    "    model = \"llama3.2\",\n",
    "    temperature = 0.1, # Low temperature for concise summary\n",
    "    num_predict = 512,\n",
    ")\n",
    "\n",
    "fetched_article = fetch_article(custom_article_url)\n",
    "messages = [\n",
    "    (\"system\", \"Add a appropriate title for the summary.Provide the title and summary with no other outputs and no other special characters.\"),\n",
    "    (\"user\", f\"Summarize this article in {sentence_count} short, concise sentences: {fetched_article}\"),\n",
    "]\n",
    "#using streaming for better performance and larger sites\n",
    "stream = llm.stream(messages)\n",
    "full = []\n",
    "for chunk in stream:\n",
    "    full.append(chunk.content)\n",
    "full_content = ''.join(full)\n",
    "print(full_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
